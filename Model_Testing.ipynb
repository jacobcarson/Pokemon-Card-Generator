{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc46ff2-cabe-4be2-8946-5da278a4e021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "import random\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import cupy as cp\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "spacy.prefer_gpu() \n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=[\"ner\", \"parser\", \"tagger\"])\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7ee7a2-fcde-4cd1-b30d-33d3d5b2c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cards = pd.read_csv(\"preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1beb297e-d302-4375-b9df-5ebcedc2d211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_29736\\1105891776.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  processed_images = torch.load(cache_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 12369 images.\n"
     ]
    }
   ],
   "source": [
    "def flat_array_to_image(flattened_array, height, width, use_gpu=True):\n",
    "    if use_gpu:\n",
    "        reshaped_array = cp.array(flattened_array).reshape((height, width, 3)).astype(cp.uint8)\n",
    "        reshaped_array = cp.asnumpy(reshaped_array)\n",
    "    else:\n",
    "        reshaped_array = np.array(flattened_array).reshape((height, width, 3)).astype(np.uint8)\n",
    "\n",
    "    image = Image.fromarray(reshaped_array)\n",
    "    return image\n",
    "\n",
    "def sanitize_flattened_array_string(array_string):\n",
    "    sanitized_string = array_string.replace(' ', ' ')\n",
    "    return sanitized_string\n",
    "height, width = 64, 64\n",
    "use_gpu = False\n",
    "\n",
    "cache_path = 'processed_large.pt'\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    # Load the cached data\n",
    "    print(\"Loading cached images...\")\n",
    "    processed_images = torch.load(cache_path)\n",
    "else:\n",
    "    print(\"Processing images...\")\n",
    "    processed_images = [\n",
    "        flat_array_to_image(np.array(ast.literal_eval(sanitize_flattened_array_string(x))), height, width, use_gpu) \n",
    "        for x in tqdm(final_cards['large_64x64'], desc='Processing large_64x64 images', position=0)\n",
    "    ]\n",
    "    torch.save(processed_images, cache_path)\n",
    "\n",
    "final_cards['large_64x64'] = processed_images\n",
    "\n",
    "print(f\"Processed {len(processed_images)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1a38b-0669-4573-9480-28b6bb5a45a5",
   "metadata": {},
   "source": [
    "cache_path = 'processed_pokemon.pt'\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    # Load the cached data\n",
    "    print(\"Loading cached images...\")\n",
    "    processed_images = torch.load(cache_path)\n",
    "else:\n",
    "    print(\"Processing images...\")\n",
    "    # Process and save the data\n",
    "    processed_images = [\n",
    "        flat_array_to_image(np.array(ast.literal_eval(sanitize_flattened_array_string(x))), height, width, use_gpu) \n",
    "        for x in tqdm(final_cards['pokemon_64x64'], desc='Processing pokemon_64x64 images', position=0)\n",
    "    ]\n",
    "    torch.save(processed_images, cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e3708-cef9-4670-851b-3d139176c4a6",
   "metadata": {},
   "source": [
    "final_cards = pd.read_parquet('file.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3990159-617a-4959-9fc3-c4baefec9996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>supertype</th>\n",
       "      <th>subtypes</th>\n",
       "      <th>hp</th>\n",
       "      <th>types</th>\n",
       "      <th>artist</th>\n",
       "      <th>rarity</th>\n",
       "      <th>set_name</th>\n",
       "      <th>large_64x64</th>\n",
       "      <th>pokemon_64x64</th>\n",
       "      <th>caption_embeddings</th>\n",
       "      <th>pokemon_intro_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>[17]</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>[9]</td>\n",
       "      <td>85</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=64x64 at ...</td>\n",
       "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
       "      <td>[-3.96634698e-01 -1.86856285e-01  2.77578920e-...</td>\n",
       "      <td>[-5.26714206e-01 -4.67754155e-01  2.97648579e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>[17]</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>[10]</td>\n",
       "      <td>85</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=64x64 at ...</td>\n",
       "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
       "      <td>[-3.51396680e-01 -1.25379905e-01  3.18560898e-...</td>\n",
       "      <td>[-3.65637928e-01 -8.96108747e-02  1.75579965e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>[0]</td>\n",
       "      <td>85</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=64x64 at ...</td>\n",
       "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
       "      <td>[-4.59370106e-01 -1.54282287e-01  2.86983043e-...</td>\n",
       "      <td>[-4.82110918e-01 -7.08584368e-01  1.12857044e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "      <td>[17]</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>[5]</td>\n",
       "      <td>118</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=64x64 at ...</td>\n",
       "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
       "      <td>[-4.22779500e-01 -1.33981556e-01  2.65981674e-...</td>\n",
       "      <td>[-3.93124610e-01 -1.14848644e-01  2.64526993e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>[0]</td>\n",
       "      <td>85</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=64x64 at ...</td>\n",
       "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
       "      <td>[-4.79958475e-01 -1.35355964e-01  3.01812291e-...</td>\n",
       "      <td>[-3.27733248e-01 -4.54510987e-01  2.78392106e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  name  supertype subtypes        hp types  artist  rarity  \\\n",
       "0           0   0    22          0     [17]  0.161290   [9]      85       7   \n",
       "1           1  11   178          0     [17]  0.225806  [10]      85       7   \n",
       "2           2  22   285          0      [1]  0.290323   [0]      85       7   \n",
       "3           3  33   287          0     [17]  0.290323   [5]     118       7   \n",
       "4           4  44   331          0      [1]  0.032258   [0]      85       7   \n",
       "\n",
       "   set_name                                        large_64x64  \\\n",
       "0         8  <PIL.Image.Image image mode=RGB size=64x64 at ...   \n",
       "1         8  <PIL.Image.Image image mode=RGB size=64x64 at ...   \n",
       "2         8  <PIL.Image.Image image mode=RGB size=64x64 at ...   \n",
       "3         8  <PIL.Image.Image image mode=RGB size=64x64 at ...   \n",
       "4         8  <PIL.Image.Image image mode=RGB size=64x64 at ...   \n",
       "\n",
       "                                       pokemon_64x64  \\\n",
       "0  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...   \n",
       "1  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...   \n",
       "2  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...   \n",
       "3  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...   \n",
       "4  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...   \n",
       "\n",
       "                                  caption_embeddings  \\\n",
       "0  [-3.96634698e-01 -1.86856285e-01  2.77578920e-...   \n",
       "1  [-3.51396680e-01 -1.25379905e-01  3.18560898e-...   \n",
       "2  [-4.59370106e-01 -1.54282287e-01  2.86983043e-...   \n",
       "3  [-4.22779500e-01 -1.33981556e-01  2.65981674e-...   \n",
       "4  [-4.79958475e-01 -1.35355964e-01  3.01812291e-...   \n",
       "\n",
       "                            pokemon_intro_embeddings  \n",
       "0  [-5.26714206e-01 -4.67754155e-01  2.97648579e-...  \n",
       "1  [-3.65637928e-01 -8.96108747e-02  1.75579965e-...  \n",
       "2  [-4.82110918e-01 -7.08584368e-01  1.12857044e-...  \n",
       "3  [-3.93124610e-01 -1.14848644e-01  2.64526993e-...  \n",
       "4  [-3.27733248e-01 -4.54510987e-01  2.78392106e-...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_cards.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25cf9e29-4e86-41db-b77f-0849b88db2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_embedding_string(embedding_str):\n",
    "    cleaned_str = re.sub(r'\\s+', ',', embedding_str.strip())\n",
    "    return cleaned_str\n",
    "\n",
    "class FinalCardsDataset(Dataset):\n",
    "    def __init__(self, dataframe, cache_dir=\"cache/\"):\n",
    "        self.data = dataframe\n",
    "\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((64, 64)),\n",
    "        ])\n",
    "        \n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "\n",
    "        self.preprocessed_data = self._load_or_cache_data()\n",
    "\n",
    "    def _load_or_cache_data(self):\n",
    "        cached_data_path = os.path.join(self.cache_dir, \"preprocessed_data.pt\")\n",
    "        if os.path.exists(cached_data_path):\n",
    "            print(\"Loading cached data.\")\n",
    "            return torch.load(cached_data_path)\n",
    "        else:\n",
    "            print(\"Preprocessing data...\")\n",
    "            preprocessed_data = self._preprocess_data()\n",
    "            torch.save(preprocessed_data, cached_data_path)\n",
    "            return preprocessed_data\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        preprocessed_data = []\n",
    "        for idx, row in self.data.iterrows():\n",
    "            try:\n",
    "\n",
    "                types = torch.tensor(ast.literal_eval(row['types']), dtype=torch.long)\n",
    "                subtypes = torch.tensor(ast.literal_eval(row['subtypes']), dtype=torch.long)\n",
    "\n",
    "                max_length = max(len(types), len(subtypes))\n",
    "                types = F.pad(types, (0, max_length - len(types)), value=0)\n",
    "                subtypes = F.pad(subtypes, (0, max_length - len(subtypes)), value=0)\n",
    "                \n",
    "                numerical_data = {\n",
    "                    'id': torch.tensor(row['id'], dtype=torch.long),\n",
    "                    'name': torch.tensor(row['name'], dtype=torch.long),\n",
    "                    'supertype': torch.tensor(row['supertype'], dtype=torch.long),\n",
    "                    'artist': torch.tensor(row['artist'], dtype=torch.long),\n",
    "                    'rarity': torch.tensor(row['rarity'], dtype=torch.long),\n",
    "                    'set_name': torch.tensor(row['set_name'], dtype=torch.long),\n",
    "                    'hp': torch.tensor(row['hp'], dtype=torch.float),\n",
    "                }\n",
    "                \n",
    "                large_64x64 = self.image_transform(row['large_64x64'])\n",
    "                \n",
    "                preprocessed_data.append({\n",
    "                    'numerical_data': numerical_data,\n",
    "                    'types': types,\n",
    "                    'subtypes': subtypes,\n",
    "                    'large_64x64': large_64x64\n",
    "                })\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Total processed data: {len(preprocessed_data)}\")\n",
    "        return preprocessed_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.preprocessed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.preprocessed_data[idx]\n",
    "\n",
    "class VisualTransformersWithAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, output_dim, image_size):\n",
    "        super(VisualTransformersWithAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.image_embed = nn.Conv2d(3, embed_dim, kernel_size=1)\n",
    "        \n",
    "        self.numerical_embed = nn.Linear(6, embed_dim)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.hp_output = nn.Linear(embed_dim, 1) \n",
    "        self.attack_output = nn.Linear(embed_dim, 1) \n",
    "    \n",
    "    def forward(self, image, numerical_data):\n",
    "        batch_size, _, height, width = image.size()\n",
    "        image_features = self.image_embed(image)\n",
    "        image_features = image_features.flatten(2).permute(2, 0, 1) \n",
    "        \n",
    "        # Numerical feature extraction\n",
    "        numerical_features = self.numerical_embed(numerical_data)\n",
    "        \n",
    "        combined_features = torch.cat([image_features.mean(dim=0), numerical_features], dim=-1)\n",
    "        \n",
    "        attentions = []\n",
    "        for layer in self.encoder_layers:\n",
    "            combined_features, attn_weights = layer(combined_features.unsqueeze(0), combined_features.unsqueeze(0), combined_features.unsqueeze(0))\n",
    "            attentions.append(attn_weights)\n",
    "        \n",
    "        hp_pred = self.hp_output(combined_features.mean(dim=0))\n",
    "        attack_pred = self.attack_output(combined_features.mean(dim=0))\n",
    "        \n",
    "        return hp_pred, attack_pred, attentions \n",
    "\n",
    "\n",
    "def multi_task_loss(predictions, targets):\n",
    "    hp_pred, attack_pred = predictions\n",
    "    hp_target, attack_target = targets\n",
    "\n",
    "    hp_loss = F.mse_loss(hp_pred.squeeze(), hp_target)\n",
    "    attack_loss = F.mse_loss(attack_pred.squeeze(), attack_target)\n",
    "    \n",
    "    total_loss = hp_loss + attack_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def visualize_attention(image, attention_weights, feature_name, save_path=None):\n",
    "    seq_len = int(np.sqrt(attention_weights.shape[0]))\n",
    "    attention_map = attention_weights.mean(0).reshape(seq_len, seq_len)  # Average across heads\n",
    "    \n",
    "    attention_map = F.interpolate(\n",
    "        torch.tensor(attention_map).unsqueeze(0).unsqueeze(0),\n",
    "        size=image.shape[:2],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    ).squeeze().detach().numpy()\n",
    "    \n",
    "    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(attention_map, cmap=\"jet\", alpha=0.5)\n",
    "    plt.title(f\"Attention Map for {feature_name}\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def generate_card(model, image, text_embeddings):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        text_embeddings = text_embeddings.to(device)\n",
    "        \n",
    "        hp_output, attack_output, attentions = model(image.unsqueeze(0), text_embeddings.unsqueeze(0))\n",
    "        return hp_output.cpu().numpy(), attack_output.cpu().numpy(), attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b50a1d0f-fcb1-4a0b-b410-1e9ada74d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_29736\\2263731795.py:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Preprocessing data...\n",
      "Total processed data: 100\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|                                                                                | 0/50 [00:00<?, ?it/s]C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_29736\\2263731795.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/3:   0%|                                                                                | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (float) and bias type (struct c10::Half) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Select the target attribute from the numerical data (e.g., 'hp')\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#targets = numerical_data.to(device, non_blocking=True)  # Example target: 'hp'\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Forward pass through the model (using only relevant input data)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Assuming that your model accepts numerical data as input\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Adjust the model to process only numerical data if needed\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 48\u001b[0m     outputs, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerical_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use numerical data as input\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Compute loss (Mean Squared Error for this example)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, numerical_data)\n",
      "File \u001b[1;32m~\\fanshawe\\NLP Project 2\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\fanshawe\\NLP Project 2\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 105\u001b[0m, in \u001b[0;36mVisualTransformersWithAttention.forward\u001b[1;34m(self, image, numerical_data)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, numerical_data):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# Image feature extraction\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     batch_size, _, height, width \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m--> 105\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (B, embed_dim, H, W)\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape to (seq_len, B, embed_dim)\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# Numerical feature extraction\u001b[39;00m\n",
      "File \u001b[1;32m~\\fanshawe\\NLP Project 2\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\fanshawe\\NLP Project 2\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\fanshawe\\NLP Project 2\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\fanshawe\\NLP Project 2\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (float) and bias type (struct c10::Half) should be the same"
     ]
    }
   ],
   "source": [
    "print(\"1\")\n",
    "dataset = FinalCardsDataset(final_cards.sample(n=100, random_state=42))\n",
    "print(\"2\")\n",
    "batch_size = min(2, len(dataset))\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "print(\"3\")\n",
    "\n",
    "embed_dim = 10\n",
    "num_heads = 5\n",
    "num_layers = 2\n",
    "output_dim = 2\n",
    "image_size = 32\n",
    "\n",
    "print(\"4\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VisualTransformersWithAttention(embed_dim, num_heads, num_layers, output_dim, image_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"5\")\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        numerical_data = batch['numerical_data']['hp']\n",
    "        image = batch['large_64x64']\n",
    "        \n",
    "        with autocast():\n",
    "            outputs, attentions = model(image, numerical_data)\n",
    "            \n",
    "            loss = criterion(outputs, numerical_data)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Avg Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc963888-b0b0-4c05-8fde-e6bbcbab61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_embedding_string(embedding_str):\n",
    "    return re.sub(r'\\s+', ',', embedding_str.strip())\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    numerical_data = [item['numerical_data']['hp'] for item in batch]\n",
    "    image_data = [item['large_64x64'] for item in batch]\n",
    "    \n",
    "    image_data = torch.stack(image_data, dim=0)\n",
    "\n",
    "    return {'numerical_data': numerical_data, 'image': image_data}\n",
    "\n",
    "class FinalCardsDataset(Dataset):\n",
    "    def __init__(self, dataframe, cache_dir=\"cache/\"):\n",
    "        self.data = dataframe\n",
    "        self.image_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((64, 64))])\n",
    "        self.cache_dir = cache_dir\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "        self.preprocessed_data = self._load_or_cache_data()\n",
    "\n",
    "    def _load_or_cache_data(self):\n",
    "        cached_data_path = os.path.join(self.cache_dir, \"preprocessed_data.pt\")\n",
    "        if os.path.exists(cached_data_path):\n",
    "            return torch.load(cached_data_path)\n",
    "        preprocessed_data = self._preprocess_data()\n",
    "        torch.save(preprocessed_data, cached_data_path)\n",
    "        return preprocessed_data\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        preprocessed_data = []\n",
    "        for idx, row in self.data.iterrows():\n",
    "            try:\n",
    "                numerical_data = torch.tensor([\n",
    "                    row['id'], row['name'], row['supertype'], row['artist'],\n",
    "                    row['rarity'], row['set_name'], row['hp']], dtype=torch.float)\n",
    "                \n",
    "                large_64x64 = self.image_transform(row['large_64x64'])\n",
    "                \n",
    "                preprocessed_data.append({\n",
    "                    'numerical_data': numerical_data,\n",
    "                    'large_64x64': large_64x64\n",
    "                })\n",
    "            except Exception:\n",
    "                continue\n",
    "        return preprocessed_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.preprocessed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.preprocessed_data[idx]\n",
    "\n",
    "class VisualTransformersWithAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=4, num_layers=2, output_dim=1, image_size=32):\n",
    "        super(VisualTransformersWithAttention, self).__init__()\n",
    "\n",
    "        self.image_embed = nn.Conv2d(3, embed_dim, kernel_size=3, stride=1, padding=1)  # Ensure output channels match embed_dim\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.fc_output = nn.Linear(embed_dim, output_dim)\n",
    "        \n",
    "    def forward(self, image, numerical_data=None):\n",
    "        image_features = self.image_embed(image).flatten(2).permute(2, 0, 1)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            image_features = layer(image_features)\n",
    "\n",
    "        output = self.fc_output(image_features.mean(dim=0))\n",
    "        return output\n",
    "\n",
    "def multi_task_loss(predictions, targets):\n",
    "    hp_pred, attack_pred = predictions\n",
    "    hp_target, attack_target = targets\n",
    "    hp_loss = F.mse_loss(hp_pred.squeeze(), hp_target)\n",
    "    attack_loss = F.mse_loss(attack_pred.squeeze(), attack_target)\n",
    "    return hp_loss + attack_loss\n",
    "\n",
    "def visualize_attention(image, attention_weights, feature_name, save_path=None):\n",
    "    seq_len = int(np.sqrt(attention_weights.shape[0]))\n",
    "    attention_map = attention_weights.mean(0).reshape(seq_len, seq_len)\n",
    "    attention_map = F.interpolate(torch.tensor(attention_map).unsqueeze(0).unsqueeze(0), size=image.shape[:2], mode=\"bilinear\", align_corners=False).squeeze().detach().numpy()\n",
    "    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(attention_map, cmap=\"jet\", alpha=0.5)\n",
    "    plt.title(f\"Attention Map for {feature_name}\")\n",
    "    plt.axis(\"off\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def generate_card(model, image, text_embeddings):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        text_embeddings = text_embeddings.to(device)\n",
    "        hp_output, attack_output, attentions = model(image.unsqueeze(0), text_embeddings.unsqueeze(0))\n",
    "        return hp_output.cpu().numpy(), attack_output.cpu().numpy(), attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "365dd3df-4333-456a-a0c4-4da80b50d266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_29736\\1754834146.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cached_data_path)\n",
      "Epoch 1/3:   0%|                                                                                | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m---> 25\u001b[0m     numerical_data \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumerical_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)  \u001b[38;5;66;03m# Move numerical data to device\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     image \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move image to device\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Forward pass without autocast\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "dataset = FinalCardsDataset(final_cards.sample(n=100, random_state=42))\n",
    "batch_size = min(2, len(dataset))\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "image_size = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = VisualTransformersWithAttention(embed_dim, num_heads, num_layers, output_dim, image_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        numerical_data = batch['numerical_data'].to(device) \n",
    "        image = batch['image'].to(device)\n",
    "        \n",
    "        outputs, attentions = model(image, numerical_data)\n",
    "    \n",
    "        loss = criterion(outputs, numerical_data)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Avg Loss: {avg_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299f431-1523-45ed-9761-64b5ffdcf88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
